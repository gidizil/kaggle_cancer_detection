{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#4286f4\">Process Documentation</h2>\n",
    "<div>Here I'll try to explain the conclusions and actions that I took during this competition</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#4286f4\">Basic Model</h3>\n",
    "<ul>\n",
    "    <li>the basic model was consisted of 2 conv + pooling layers. Followed by 3 feed forward layers. </li>\n",
    "    <li>After each conv layer we used a 2D batch normalization</li>\n",
    "    <li>ReLU was used as the activation function for all of the layers</li>\n",
    "    <li>Adam Optimizer with learning rate of $3e$<sup>$-4$</sup></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><u>Model Overfitting:</u></div>\n",
    "<div>Under default hyperparameters the model overfitted very quickly</div>\n",
    "<img src='train_val_loss_basic.png' height=500 width=500 align=left></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#4286f4\">Avoid Overfitting</h3>\n",
    "<div>Exploring the affect of different methods on learning process for this basic model</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><u>Decrease Capacity 1: Remove One Linear Layer</u></div>\n",
    "<div>Remove one Feed Forward (linear) layer to decrease model capacity</div>\n",
    "<img src='train_val_loss_basic_remove_lin_layer', height=500, width=500, align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> It can be seen that it only got things worse in terms of validation loss.</div>\n",
    "<div>Overfitting starts at pretty much the same time. In addition fluctuations have been greatly increased</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><u>Regularization 1: Dropout</u></div>\n",
    "<div>Adding Dropout layers on the Feed Forward layers</div>\n",
    "<img src='train_val_loss_basic_dropout.png' height=500 width=500 align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:#4286f4\"><u>Avoid Overfitting: Conclusions</u></h4>\n",
    "<ol>\n",
    "    <li>Dropout works well</li>\n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>We can clearly see that train loss is much higher but \n",
    "the validation loss is lower with respect to without dropout.</div> \n",
    "<div>We can see that overfitting starts at the 25th epoch</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><u>Validation Accuracy:</u></div>\n",
    "<p> We can see that there is no real learning going on - Very volatile</p>\n",
    "<img src='val_accuracy_basic_dropout.png' height=500, width=500 align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><b><u>Conclusion:</u></b></div>\n",
    "<div>Train model with a known model and see if it is learning. This will help us understand if the problem is the model or the data pipeline</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><b><u>Findings:</u></b></div>\n",
    "<div>We've Chosen ResNet50 as our model, and trained it. The Learning was still not working well.</div>\n",
    "<div>So we've fixed two things that we're related to the the output of the model and the labeling:</div>\n",
    "    <ol>\n",
    "        <li>Changed from Cross-Entropy to Binary Cross-Entropy Loss</li>\n",
    "        <li>Fixed the Accuracy calculation to match the new output of the model</li>\n",
    "    </ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:#4286f4\">Avoid Overfitting: Next Ideas</h4>\n",
    "<ul>\n",
    "    <li>Make sure that <i>fit_and_eval()</i> works well - eval should update well - <span style=\"color:#4286f4\">\n",
    "        Done!</span></li>\n",
    "    <li>Decay learning rate based on validation score</li>\n",
    "    <li>Use the accuracy as a measure - Maybe accuracy is good at the 25th epoch - <span style=\"color:#4286f4\">\n",
    "        Done!</span></li>\n",
    "    \n",
    "</ul>\n",
    "<div>In addition. Here is a list of helpful places to look at:</div>\n",
    "<a href=https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607 target='_blank'>Useful Ideas</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><u>New Results: Basic Dropout Model</u></div>\n",
    "<img src='./val_accuracy_basic_w_dropout.png' height=500 width=500 align=left> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><u>New Results: ResNet50 Transfer Learning</u></div>\n",
    "<img src='./val_accuracy_resnet50.png' height=500 width=500 align=left> </img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><u>Conclusions from the models</u></div>\n",
    "<ol>\n",
    "    <li>Basic Model can't seem to be able to learn well</li>\n",
    "    <li>Resnet50 Model seems to be able to learn</li>\n",
    "</ol>\n",
    "<div style=\"color:#a03cde\"><u><b>Conclusion: </b></u>We should try to increase the model capacity with more conv layers. <b>The problem is the opposite from what I initially saught</b></div>\n",
    "<div style=\"color:#a03cde\"><b>Even low capacity models tend to overfit at some stage</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><u>Three Layers Model Performance</u></div>\n",
    "<table><tr><td><img src='val_accuracy_3_layer.png'></td><td><img src='train_val_loss_3_layers.png'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><u>Four Layers Model Performance</u></div>\n",
    "<table><tr><td><img src='val_accuracy_4_layers.png'></td><td><img src='train_val_loss_4_layers.png'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#a03cde\"><u><b>Conclusions: </b></u>\n",
    "<ol>\n",
    "    <li> Four layers model does not add value in terms of accuracy</li>\n",
    "    <li> Four layer model looks a lot more stable in terms of validation volatility</li>\n",
    "    <li> In both models most of the learning happens in the first 10 epochs</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "<div style=\"color:#a03cde\"><u><b>Ideas:</b></u>\n",
    "    <ul>\n",
    "        <li> When Validation loss starts to increase, reduce learning rate</li>\n",
    "        <li> Build a transform that randomly decide whether to augoment onr not</li>\n",
    "        <li> Note: Cropping after rotation beacuse of black edges</li>\n",
    "        <li> Perhaps start learning from augmentations only after 10 epochs can reduce training time</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#4286f4\">Augmentations</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><u>Augmentations - 1</u></div>\n",
    "<div>Applying random augmentations as way to increase variance in training set and in order to regularize</div>\n",
    "<table><tr><td><img src='val_accuracy_4_layers_augment.png'></td><td><img src='train_val_loss_4_layers_augment.png'></td></tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><u>Augmentations - 2</u></div>\n",
    "<div>More elaborate way to integrate between original and augmented data</div>\n",
    "<table><tr><td><img src='val_accuracy_4_layers_augment_v2.png'></td><td><img src='train_val_loss_4_layers_augment_v2.png'></td></tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#a03cde\"><u><b>Conclusions: </b></u>\n",
    "<ol>\n",
    "    <li> Learning is not nearly as stable in the first scenario</li>\n",
    "    <li> Learning in the second scenario learning is more stable</li>\n",
    "    <li> Validation loss keeps decreasing in the first scenario</li>\n",
    "    <li> Learning in the second scenario stops after about 30 epochs</li>\n",
    "    <li> Training loss goes much slower with respect to no augmentations</li>\n",
    "    <li> There seems to be a pattern in the validation loss</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "<div style=\"color:#a03cde\"><u><b>Ideas:</b></u>\n",
    "    <ul>\n",
    "        <li> Increase Number of epochs for augmentations</li>\n",
    "        <li> Does the Validation loss pattern means anything</li>\n",
    "        <li> Use the mix of augment and non-augment in favor of augment</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><u>Augmentations - 3</u></div>\n",
    "<div>More epochs and $[2/3, 1/3]$ ratio of augmented batches. Added color jitter to augmentation</div>\n",
    "<table><tr><td><img src='val_accuracy_4_layers_augment_mixed.png'></td><td><img src='train_val_loss_4_layers_augment_mixed.png'></td></tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><u>Augmentations - 4</u></div>\n",
    "<div> f conv layers model and 80 epochs of only augmented images. Added color jitter to augmentation</div>\n",
    "<table><tr><td><img src='val_accuracy_augment_80_epochs.png'></td><td><img src='train_val_loss_augment_80_epochs.png'></td></tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><u>Augmentations - 5</u></div>\n",
    "<div>Different augmentations for training and validation. Same training as augmentations 3</div>\n",
    "<table><tr><td><img src='val_accuracy_diff_augment.png'></td><td><img src='train_val_loss_diff_augment.png'></td></tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><u>Augmentations - 6</u></div>\n",
    "<div>Different augmentations for training and validation. Same training as augmentations 1</div>\n",
    "<table><tr><td><img src='val_accuracy_diff_augment_tr_augment_only.png'></td><td><img src='train_val_loss_diff_augment_tr_augment_only.png'></td></tr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#a03cde\"><u><b>Conclusions: </b></u>\n",
    "<ol>\n",
    "    <li> Best result where showed with augmentation only and color jitter dosen't matter that much</li>\n",
    "    <li> Learning with augmentation produces less stable learning </li>\n",
    "    <li> randomness is a welcome guest of the learning process:)</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "<div style=\"color:#a03cde\"><u><b>Ideas:</b></u>\n",
    "    <ul>\n",
    "        <li> Increase randomness by decreasing batch size</li>\n",
    "        <li> Split training to augmented and non-augmented loader. Start with the non-augmented one</li>\n",
    "        <li> Use the mix of augment and non-augment in favor of augment</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#4286f4\"><u>Decrease Batch Size</u></div>\n",
    "<div>Same training as augmentations 6. Using a batch size of 64 instead of 256</div>\n",
    "<table><tr><td><img src='val_accuracy_augment_64_batch.png'></td><td><img src='train_val_loss_augment_64_batch.png'></td></tr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>Interesting phenomena where validation loss is constatntly better than training loss</li>\n",
    "    <li>No advantage came from reducing batch size in terms of validation accuracy</li>\n",
    "    <li>Learning happens much faster in first few epochs</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#a03cde\"><u><b>Augmentations Conclusions: </b></u></div>\n",
    "<div style=\"color:#a03cde\">Seems like we've pretty much hit the surface. We will continue with augmentations 6.</div> <div style=\"color:#a03cde\">Now Moving on to Gradient descent algorithms and tuning their parameters</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#4286f4\">Gradient Descent Tuning</h3>\n",
    "<div>Taking a step back and talking about hyperparameters tuning</div>\n",
    "<div> Here we will learn how to:\n",
    "<ol>\n",
    "    <li> Using the different gradient descent methods</li>\n",
    "    <li> How to tune hyperparameters of all some methods</li>\n",
    "    <li> Incorparate between different methods in different time of training</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
